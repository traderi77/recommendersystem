{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "words = open('names.txt', 'r').read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.1239, grad_fn=<NllLossBackward0>)\n",
      "tensor(14.5315, grad_fn=<NllLossBackward0>)\n",
      "tensor(14.0415, grad_fn=<NllLossBackward0>)\n",
      "tensor(13.5751, grad_fn=<NllLossBackward0>)\n",
      "tensor(13.1320, grad_fn=<NllLossBackward0>)\n",
      "tensor(12.7185, grad_fn=<NllLossBackward0>)\n",
      "tensor(12.3441, grad_fn=<NllLossBackward0>)\n",
      "tensor(12.0134, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.7189, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.4494, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.1982, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.9619, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.7384, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.5262, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.3241, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.1313, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.9467, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.7698, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.5999, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.4363, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.2785, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.1263, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.9791, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.8367, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.6987, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.5651, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.4355, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.3098, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.1878, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.0694, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.9544, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8428, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7343, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6289, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5265, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4269, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3300, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2357, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1439, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0544, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.9673, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.8823, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.7994, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.7185, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.6396, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.5625, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.4873, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.4138, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3420, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2719, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2033, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1363, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0707, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0066, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9437, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8821, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8218, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7625, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7044, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6472, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5911, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5359, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4816, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4281, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3755, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3237, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2727, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2224, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1729, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1242, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.0762, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.0290, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9826, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9370, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8923, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8484, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8053, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7632, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7220, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6818, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6425, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6042, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5669, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5306, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4952, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4608, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4273, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3948, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3631, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3323, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3024, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2734, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2451, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2177, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1910, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1650, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1398, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1153, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0914, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0682, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {val: index+1 for index, val in enumerate(chars)}\n",
    "itos = {index+1: val for index, val in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "BLOCK_SIZE = 3\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "X, Y = [], []\n",
    "for w in words: \n",
    "    context = [0] * BLOCK_SIZE\n",
    "    for ch in w + '.': \n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix) \n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "C = torch.randn((27, 2), requires_grad=True) \n",
    "W1 = torch.randn((6, 100), requires_grad=True) \n",
    "b1 = torch.randn(100, requires_grad=True)\n",
    "W2 = torch.randn((100, 27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True) \n",
    "\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "for i in range(100): \n",
    "    emb = C[X]  \n",
    "\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss)\n",
    "\n",
    "    for param in params: \n",
    "        param.grad = None    \n",
    "\n",
    "    loss.backward() \n",
    "\n",
    "    for param in params:      \n",
    "        param.data += -LEARNING_RATE * param.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuer df mit \n",
    "\n",
    "session_id, sap_gp_id, action, product_id\n",
    "\n",
    "\n",
    "actions:\n",
    "1: search -> dafür muss wkn inferiert werden aus top-k ergebnissen \n",
    "2: trx-simulation -> add interaction der wkn \n",
    "3: rfo/rfq -> anfrage der wkn, anfangs unabhängig von order fill\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_nets_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
